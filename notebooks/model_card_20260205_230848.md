
    # Model Card: XGBoost (Optuna)

    ## Model Information
    - Model Type: XGBoost (Optuna)
    - Training Date: 20260205_230848
    - Random State: 42

    ## Dataset
    - Total Samples: 2845
    - Training Samples: 1991
    - Validation Samples: 427
    - Test Samples: 427
    - Features: ipv, ips, iaa, ieg, nยบ_av, ida, media
    - Target: defasagem (binary: 0/1)
    - Class Imbalance Ratio: 0.69

    ## Performance Metrics (Test Set)
    - Recall: 0.929
    - Precision: 0.886
    - F1-Score: 0.907
    - ROC-AUC: 0.956
    - PR-AUC: 0.967

    ## Hyperparameters
    {
  "n_estimators": 488,
  "max_depth": 3,
  "learning_rate": 0.2733963728804307,
  "subsample": 0.6864850757631904,
  "colsample_bytree": 0.8744098046085843,
  "min_child_weight": 1,
  "gamma": 0.19250513125261576,
  "reg_alpha": 0.4297478407837856,
  "reg_lambda": 0.0951926550865545,
  "scale_pos_weight": 0.694468085106383,
  "random_state": 42,
  "eval_metric": "logloss",
  "use_label_encoder": false
}

    ## Usage
```python
    import joblib
    model = joblib.load('best_model_XGBoost_(Optuna)_20260205_230848.pkl')
    predictions = model.predict(X)
    probabilities = model.predict_proba(X)[:, 1]
```

    ## Notes
    - Model optimized for recall (minimum 0.85 requirement)
    - Suitable for imbalanced classification problems
    - Consider recalibration if probability estimates are critical
    