
    # Model Card: XGBoost (Optuna)

    ## Model Information
    - Model Type: XGBoost (Optuna)
    - Training Date: 20260212_221804
    - Random State: 42

    ## Dataset
    - Total Samples: 2845
    - Training Samples: 1991
    - Validation Samples: 427
    - Test Samples: 427
    - Features: ipv, ips, iaa, ieg, no_av, ida, media
    - Target: defasagem (binary: 0/1)
    - Class Imbalance Ratio: 0.69

    ## Performance Metrics (Test Set)
    - Recall: 0.909
    - Precision: 0.905
    - F1-Score: 0.907
    - ROC-AUC: 0.946
    - PR-AUC: 0.957

    ## Hyperparameters
    {
  "n_estimators": 491,
  "max_depth": 3,
  "learning_rate": 0.29955720263572994,
  "subsample": 0.642469645175394,
  "colsample_bytree": 0.9378744522176313,
  "min_child_weight": 2,
  "gamma": 0.4468785979783484,
  "reg_alpha": 0.45072828540091214,
  "reg_lambda": 0.07933724875824494,
  "scale_pos_weight": 0.694468085106383,
  "random_state": 42,
  "eval_metric": "logloss",
  "use_label_encoder": false
}

    ## Usage
```python
    import joblib
    model = joblib.load('best_model_XGBoost_(Optuna)_20260212_221804.pkl')
    predictions = model.predict(X)
    probabilities = model.predict_proba(X)[:, 1]
```

    ## Notes
    - Model optimized for recall (minimum 0.85 requirement)
    - Suitable for imbalanced classification problems
    - Consider recalibration if probability estimates are critical
    